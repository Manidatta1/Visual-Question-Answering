---
title: "Visual Question Answering with Multimodal Architectures"
description: "VQA system using answerability classification and answer generation models on the VizWiz dataset"
---

# ğŸ¤– Visual Question Answering with Multimodal Architectures

This project presents a **Visual Question Answering (VQA)** system using two distinct multimodal deep learning models. It uses both images and natural language questions to either:
- â“ Predict whether a question is answerable (Binary classification)
- ğŸ—£ï¸ Generate the best answer from 301 predefined classes (Multiclass classification)

Built and evaluated on the **real-world VizWiz dataset**, this project demonstrates attention-based fusion and fine-tuned CNNs for robust image-question reasoning.

---

## ğŸ§  Model Architectures

### 1. ğŸ” Answerability Classification Model

A binary classifier that predicts if a given image-question pair is answerable.

**Architecture:**

- **Image Pipeline**
  - CNN with 4 Conv2d blocks + MaxPool, BatchNorm, ReLU
  - Flatten â†’ Linear layer (32768 â†’ hidden_dim)
- **Question Pipeline**
  - GloVe vector (50d) â†’ Linear â†’ ReLU + Dropout
- **Cross Attention**
  - Multi-headed attention (Image as Query, Question as Key/Value)
- **Fusion & Classification**
  - FC â†’ ReLU + Dropout â†’ FC â†’ Sigmoid

**Input**: Image tensor `(3Ã—128Ã—128)`, GloVe question embedding `(50-d)`  
**Output**: Binary logit â†’ Sigmoid â†’ [0, 1]

---

### 2. âœ¨ Answer Generation Model

Multiclass classifier that predicts one of the **301** answer classes.

**Architecture:**

- **Image Pipeline**
  - Fine-tuned **ResNet18** (last 4 layers only)
  - FC â†’ ReLU + Dropout (x2)
- **Question Pipeline**
  - GloVe vector â†’ FC â†’ ReLU + Dropout
- **Cross Attention**
  - Multi-head attention (Image as Query, Question as Key/Value)
- **Fusion & Classification**
  - Concatenate attended features â†’ FC â†’ LayerNorm â†’ ReLU + Dropout (x2)
  - FC â†’ 301 class logits

**Input**: Image tensor `(3Ã—128Ã—128)`, GloVe vector  
**Output**: 301 class scores (softmax during evaluation)

---

## ğŸ§ª Model Performance

| Model                    | Accuracy |
|--------------------------|----------|
| Answerability Classifier | **60.0%** |
| Answer Generator         | **55.3%** |


No pre-trained language models (e.g., BERT, GPT) were used, demonstrating the effectiveness of lightweight, interpretable multimodal fusion techniques in VQA settings.

---

## ğŸ”§ Hyperparameter Tuning (Optuna)

| Model                    | Best Params |
|--------------------------|-------------|
| Answerability Classifier | Adam, LR: `0.0048`, Hidden Dim: `512`, Batch: `32` |
| Answer Generator         | Adam, LR: `0.00249`, Hidden Dim: `512`, Batch: `16`, Heads: `4` |

**Trends:**
- ResNet18 transfer learning boosted accuracy
- More attention heads improved multimodal representation
- Smaller batch sizes improved generalization
- Adam outperformed SGD and AdamW

---

## ğŸ› ï¸ Technologies Used

- Python, PyTorch
- ResNet18 (torchvision)
- GloVe Embeddings (glove.6B.50d)
- spaCy (text preprocessing)
- Optuna (hyperparameter optimization)
- NumPy, Matplotlib, Pandas

---

## ğŸ§‘â€ğŸ’» Author

**ManiDatta**  
Masterâ€™s in Data Science @ University of Colorado Boulder  
[GitHub](https://github.com/Manidatta1)
