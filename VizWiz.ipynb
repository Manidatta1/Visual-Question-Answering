{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for the images\n",
    "image_directory = 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/'\n",
    "\n",
    "# Directory for the annotations\n",
    "annotation_directory= 'https://vizwiz.cs.colorado.edu/VizWiz_final/vqa_data/Annotations/'\n",
    "\n",
    "train_annotation_path = '{}{}'.format(annotation_directory, 'train.json')\n",
    "val_annotation_path = '{}{}'.format(annotation_directory, 'val.json')\n",
    "test_annotation_path = '{}{}'.format(annotation_directory, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 20523\n",
      "Validation set size: 4319\n",
      "Test set size: 8000\n"
     ]
    }
   ],
   "source": [
    "# Train annotations\n",
    "train_data = requests.get(train_annotation_path, allow_redirects=True)\n",
    "train_data_n = train_data.json()\n",
    "\n",
    "# Validation annotations\n",
    "val_data = requests.get(val_annotation_path, allow_redirects=True)\n",
    "val_data_n = val_data.json()\n",
    "\n",
    "# Test annotations\n",
    "test_data = requests.get(test_annotation_path, allow_redirects=True)\n",
    "test_data_n = test_data.json()\n",
    "\n",
    "print('Train set size:', len(train_data_n))\n",
    "print('Validation set size:', len(val_data_n))\n",
    "print('Test set size:', len(test_data_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_image_urls = [\n",
    "    image_directory + sample[\"image\"]\n",
    "    for sample in test_data_n[:100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# splitting the data \n",
    "def train_val_split(data,size):\n",
    "    data = list(data)\n",
    "\n",
    "    class_1 = [sample for sample in data if sample['answerable'] == 1]\n",
    "    class_0 = [sample for sample in data if sample['answerable'] == 0]\n",
    "\n",
    "    available_1, available_0 = len(class_1), len(class_0)\n",
    "\n",
    "    num_samples_per_class = min(size // 2, available_1, available_0)\n",
    "\n",
    "    sampled_class_1 = random.sample(class_1,num_samples_per_class)\n",
    "    sampled_class_0 = random.sample(class_0, num_samples_per_class)\n",
    "\n",
    "    balanced_data = sampled_class_1 + sampled_class_0\n",
    "    random.shuffle(balanced_data)\n",
    "\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 6000 samples from the train data\n",
    "train_data = train_data_n[:2000]\n",
    "\n",
    "# Taking 300 samples from the val data\n",
    "val_data = val_data_n[:300]\n",
    "\n",
    "# Taking 100 samples from the test data\n",
    "test_data = test_data_n[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 6000 samples from the train data\n",
    "train_data_ac = train_val_split(train_data_n,2000)\n",
    "\n",
    "# Taking 300 samples from the val data\n",
    "val_data_ac = train_val_split(val_data_n,300)\n",
    "\n",
    "# Taking 100 samples from the test data\n",
    "test_data_ac = test_data_n[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preorocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "\n",
    "def Image_tensors(data):\n",
    "    transform = transforms.Compose([\n",
    "    # Resizing the image\n",
    "    transforms.Resize((128,128)),\n",
    "    # Converting the images to tensor \n",
    "    transforms.ToTensor(),\n",
    "    # Applying the Normalization and Standardization\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Applying the above defined transformations to all the images\n",
    "\n",
    "    # All the transformed images are stored in image_tensors list\n",
    "    image_tensors = []\n",
    "    for images in data:\n",
    "        image_url = image_directory + images[\"image\"]\n",
    "        # making the HTTP get request to fetch the image which returns the binary image data\n",
    "        response = requests.get(image_url)\n",
    "        # converting the binary image data into the image\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img_preprocessed = transform(img)\n",
    "        image_tensors.append(img_preprocessed)\n",
    "    image_tensors = torch.stack(image_tensors)\n",
    "    return image_tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import contractions\n",
    "import torch\n",
    "\n",
    "# Loading the  Spacy tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_question(question):\n",
    "    # converting the questions to lower case\n",
    "    question = question.lower()\n",
    "    # expanding the contractions : what's -> what is\n",
    "    question = contractions.fix(question)\n",
    "    # removing the special characters \n",
    "    question = re.sub(r\"[^a-zA-Z\\s]\", \"\", question)\n",
    "    doc = nlp(question)\n",
    "    # applying the lemmatization and removing the stop words and punctuations\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "def get_glove_embedding_for_question(question, glove_dict, embedding_dim=50):\n",
    "    tokens = preprocess_question(question)\n",
    "    vectors = [glove_dict[token] for token in tokens if token in glove_dict]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        return torch.zeros(embedding_dim)\n",
    "    else:\n",
    "        return torch.mean(torch.stack(vectors), dim=0)\n",
    "\n",
    "def Question_Tensors(data, glove_dict, embedding_dim=50):\n",
    "    questions = [sample[\"question\"] for sample in data]\n",
    "    embeddings = [get_glove_embedding_for_question(q, glove_dict, embedding_dim) for q in questions]\n",
    "    question_tensor = torch.stack(embeddings)\n",
    "    return question_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_glove_embeddings(path='glove.6B.50d.txt'):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"GloVe file is not found at the path: {path}\")\n",
    "    \n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float32)\n",
    "                embeddings[word] = vector\n",
    "            except ValueError:\n",
    "                print(f\"Skipping the corrupted line: {line[:50]}...\")\n",
    "                continue\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the top 300 answers, assigning the id's to them and treating remaning answers as other_categories \n",
    "from collections import Counter\n",
    "top_n = 300\n",
    "def choosen_answers(data):\n",
    "    chosen_answers = []\n",
    "\n",
    "    # Choosing the most common answer from the ten lables for each sample\n",
    "    for sample in data:\n",
    "        answers = [entry['answer'] for entry in sample['answers']]\n",
    "        answer_counts = Counter(answers) \n",
    "        top_answer, _ = answer_counts.most_common(1)[0] \n",
    "        chosen_answers.append(top_answer)\n",
    "\n",
    "\n",
    "    answer_counts = Counter(chosen_answers)\n",
    "    top_answers = answer_counts.most_common(top_n) \n",
    "\n",
    " \n",
    "\n",
    "    # Create categories for top n\n",
    "    category_name2id = {answer:ind for ind, (answer, _) in enumerate(top_answers)}\n",
    "\n",
    "    category_id2name = {ind:answer for ind, (answer, _) in enumerate(top_answers)}\n",
    "    \n",
    "\n",
    "    \n",
    "    category_id2name[top_n] = 'other_categories'\n",
    "    return chosen_answers,category_id2name,category_name2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Target_tensors(chosen_answers,category_name2id):\n",
    "    targets = []\n",
    "\n",
    "    for ans in chosen_answers:\n",
    "        if ans in category_name2id.keys():\n",
    "            targets.append(category_name2id[ans])\n",
    "        else:\n",
    "            targets.append(top_n) \n",
    "\n",
    "    targets_tensor = torch.tensor(targets)\n",
    "    return targets_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#train_image_tensors_ac = Image_tensors(train_data_ac)\n",
    "#val_image_tensors_ac = Image_tensors(val_data_ac)\n",
    "#test_image_tensors_ac = Image_tensors(test_data_ac)\n",
    "\n",
    "#torch.save(train_image_tensors_ac, \"train_image_tensors_ac.pt\")\n",
    "#torch.save(val_image_tensors_ac, \"val_image_tensors_ac.pt\")\n",
    "#torch.save(test_image_tensors_ac, \"test_image_tensors_ac.pt\")\n",
    "\n",
    "\n",
    "\n",
    "glove_dict = load_glove_embeddings(\"glove.6B.50d.txt\")\n",
    "embedding_dim = 50\n",
    "\n",
    "\n",
    "train_question_tensors_ac = Question_Tensors(train_data_ac, glove_dict, embedding_dim)\n",
    "val_question_tensors_ac = Question_Tensors(val_data_ac, glove_dict, embedding_dim)\n",
    "test_question_tensors_ac = Question_Tensors(test_data_ac, glove_dict, embedding_dim)\n",
    "\n",
    "torch.save(train_question_tensors_ac, \"train_question_tensors_ac.pt\")\n",
    "torch.save(val_question_tensors_ac, \"val_question_tensors_ac.pt\")\n",
    "torch.save(test_question_tensors_ac, \"test_question_tensors_ac.pt\")\n",
    "\n",
    "\n",
    "train_image_tensors_ac = torch.load(\"train_image_tensors_ac.pt\")\n",
    "val_image_tensors_ac= torch.load(\"val_image_tensors_ac.pt\")\n",
    "test_image_tensors_ac= torch.load(\"test_image_tensors_ac.pt\")\n",
    "\n",
    "train_question_tensors_ac = torch.load(\"train_question_tensors_ac.pt\")\n",
    "val_question_tensors_ac= torch.load(\"val_question_tensors_ac.pt\")\n",
    "test_question_tensors_ac= torch.load(\"test_question_tensors_ac.pt\")\n",
    "\n",
    "def get_answerability_labels(data):\n",
    "    return torch.tensor([sample[\"answerable\"] for sample in data], dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_labels = get_answerability_labels(train_data_ac)\n",
    "val_labels = get_answerability_labels(val_data_ac)\n",
    "\n",
    "\n",
    "train_dataset_ac = TensorDataset(train_image_tensors_ac,train_question_tensors_ac,train_labels)\n",
    "val_dataset_ac = TensorDataset(val_image_tensors_ac,val_question_tensors_ac,val_labels)\n",
    "test_dataset_ac = TensorDataset(test_image_tensors_ac,test_question_tensors_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Answerable Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        return attn_output\n",
    "\n",
    "class answerclassifier(nn.Module):\n",
    "    def __init__(self,num_classes=1, hidden_dim=512, dropout_prob=0.1, num_heads=4):\n",
    "        super(answerclassifier, self).__init__()\n",
    "\n",
    "        # Image feature extractor\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc_image = nn.Linear(512 * 8 * 8, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Question feature extractor\n",
    "        self.fc_question = nn.Linear(50, hidden_dim)\n",
    "  \n",
    "\n",
    "        # Cross attention\n",
    "        self.cross_attention = CrossAttention(embed_dim=hidden_dim, num_heads=num_heads)\n",
    "\n",
    "        # Fusion + classification\n",
    "        self.fc_fusion = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, image, question_embedding):\n",
    "        # Image pipeline\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(image))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        image_features = self.dropout(F.relu(self.fc_image(x)))\n",
    "\n",
    "        # Question pipeline (assuming question is already an embedding)\n",
    "        question_features = self.dropout(F.relu(self.fc_question(question_embedding)))\n",
    "\n",
    "        # Adding the sequence dim for cross-attention\n",
    "        image_seq = image_features.unsqueeze(1)  \n",
    "        question_seq = question_features.unsqueeze(1)  \n",
    "\n",
    "        attended_features = self.cross_attention(query=image_seq, key=question_seq, value=question_seq)\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "\n",
    "        # Fusion and classification\n",
    "        fused = self.dropout(F.relu(self.fc_fusion(attended_features)))\n",
    "        output = self.fc_output(fused)\n",
    "        return output.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:32:09,174] A new study created in memory with name: no-name-82016b18-0385-4c92-b303-5229be17a7a9\n",
      "/var/folders/_h/ngxdgxln57gb923874j8w4rh0000gn/T/ipykernel_96964/3194419313.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\",1e-3, 5e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trail 0 \n",
      "\n",
      "Selected Hyperparameters: lr=0.002523507274395066, batch_size=32, hidden_dim=512, optimizer=AdamW\n",
      "Epoch [1/5] - Train Loss: 0.6784, Val Loss: 0.6746, Val Acc: 0.5867\n",
      "Epoch [2/5] - Train Loss: 0.6727, Val Loss: 0.6619, Val Acc: 0.6000\n",
      "Epoch [3/5] - Train Loss: 0.6509, Val Loss: 0.6727, Val Acc: 0.5867\n",
      "Epoch [4/5] - Train Loss: 0.6556, Val Loss: 0.6691, Val Acc: 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:35:25,370] Trial 0 finished with value: 0.6 and parameters: {'learning_rate': 0.002523507274395066, 'batch_size': 32, 'hidden_dimension': 512, 'optimizer': 'AdamW'}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - Train Loss: 0.6341, Val Loss: 0.6604, Val Acc: 0.6000\n",
      "Trial 0 Completed!  Val Acc: 0.6000\n",
      "\n",
      "Running Trail 1 \n",
      "\n",
      "Selected Hyperparameters: lr=0.0016384618842067798, batch_size=32, hidden_dim=256, optimizer=Adam\n",
      "Epoch [1/5] - Train Loss: 0.6684, Val Loss: 0.6564, Val Acc: 0.5833\n",
      "Epoch [2/5] - Train Loss: 0.6461, Val Loss: 0.6629, Val Acc: 0.5900\n",
      "Epoch [3/5] - Train Loss: 0.6393, Val Loss: 0.6796, Val Acc: 0.5967\n",
      "Epoch [4/5] - Train Loss: 0.6339, Val Loss: 0.6774, Val Acc: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:38:35,292] Trial 1 finished with value: 0.5966666666666667 and parameters: {'learning_rate': 0.0016384618842067798, 'batch_size': 32, 'hidden_dimension': 256, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - Train Loss: 0.6318, Val Loss: 0.6796, Val Acc: 0.5933\n",
      "Trial 1 Completed!  Val Acc: 0.5967\n",
      "\n",
      "Running Trail 2 \n",
      "\n",
      "Selected Hyperparameters: lr=0.00483184599739595, batch_size=32, hidden_dim=512, optimizer=Adam\n",
      "Epoch [1/5] - Train Loss: 0.7360, Val Loss: 0.7365, Val Acc: 0.4467\n",
      "Epoch [2/5] - Train Loss: 0.6959, Val Loss: 0.6743, Val Acc: 0.5900\n",
      "Epoch [3/5] - Train Loss: 0.6728, Val Loss: 0.6802, Val Acc: 0.5367\n",
      "Epoch [4/5] - Train Loss: 0.6507, Val Loss: 0.6390, Val Acc: 0.6167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:41:51,995] Trial 2 finished with value: 0.6166666666666667 and parameters: {'learning_rate': 0.00483184599739595, 'batch_size': 32, 'hidden_dimension': 512, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.6166666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - Train Loss: 0.6663, Val Loss: 0.6749, Val Acc: 0.5900\n",
      "Trial 2 Completed!  Val Acc: 0.6167\n",
      "\n",
      "Running Trail 3 \n",
      "\n",
      "Selected Hyperparameters: lr=0.0016390249952238879, batch_size=64, hidden_dim=256, optimizer=AdamW\n",
      "Epoch [1/5] - Train Loss: 0.6746, Val Loss: 0.6531, Val Acc: 0.6067\n",
      "Epoch [2/5] - Train Loss: 0.6476, Val Loss: 0.6579, Val Acc: 0.6033\n",
      "Epoch [3/5] - Train Loss: 0.6403, Val Loss: 0.6505, Val Acc: 0.5900\n",
      "Epoch [4/5] - Train Loss: 0.6326, Val Loss: 0.6515, Val Acc: 0.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:44:59,773] Trial 3 finished with value: 0.6133333333333333 and parameters: {'learning_rate': 0.0016390249952238879, 'batch_size': 64, 'hidden_dimension': 256, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.6166666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - Train Loss: 0.6211, Val Loss: 0.6707, Val Acc: 0.5433\n",
      "Trial 3 Completed!  Val Acc: 0.6133\n",
      "\n",
      "Running Trail 4 \n",
      "\n",
      "Selected Hyperparameters: lr=0.001195410810510843, batch_size=32, hidden_dim=512, optimizer=Adam\n",
      "Epoch [1/5] - Train Loss: 0.6785, Val Loss: 0.6517, Val Acc: 0.6067\n",
      "Epoch [2/5] - Train Loss: 0.6495, Val Loss: 0.6533, Val Acc: 0.6033\n",
      "Epoch [3/5] - Train Loss: 0.6389, Val Loss: 0.6680, Val Acc: 0.6100\n",
      "Epoch [4/5] - Train Loss: 0.6260, Val Loss: 0.6561, Val Acc: 0.6167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:48:12,745] Trial 4 finished with value: 0.6166666666666667 and parameters: {'learning_rate': 0.001195410810510843, 'batch_size': 32, 'hidden_dimension': 512, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.6166666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - Train Loss: 0.6141, Val Loss: 0.7073, Val Acc: 0.5933\n",
      "Trial 4 Completed!  Val Acc: 0.6167\n",
      "\n",
      "Running Trail 5 \n",
      "\n",
      "Selected Hyperparameters: lr=0.004202118479167341, batch_size=64, hidden_dim=512, optimizer=SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:48:49,710] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train Loss: 0.6933, Val Loss: 0.6925, Val Acc: 0.5000\n",
      "Running Trail 6 \n",
      "\n",
      "Selected Hyperparameters: lr=0.0037993259355611733, batch_size=32, hidden_dim=256, optimizer=SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:49:26,489] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train Loss: 0.6933, Val Loss: 0.6918, Val Acc: 0.4967\n",
      "Running Trail 7 \n",
      "\n",
      "Selected Hyperparameters: lr=0.00470294949561668, batch_size=32, hidden_dim=256, optimizer=Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:50:04,915] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train Loss: 0.6761, Val Loss: 0.6889, Val Acc: 0.5600\n",
      "Running Trail 8 \n",
      "\n",
      "Selected Hyperparameters: lr=0.0035005803719305026, batch_size=32, hidden_dim=512, optimizer=Adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:50:44,264] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train Loss: 0.6936, Val Loss: 0.6737, Val Acc: 0.5733\n",
      "Running Trail 9 \n",
      "\n",
      "Selected Hyperparameters: lr=0.002426924100418565, batch_size=64, hidden_dim=512, optimizer=Adam\n",
      "Epoch [1/5] - Train Loss: 0.6773, Val Loss: 0.6560, Val Acc: 0.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 22:51:57,940] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] - Train Loss: 0.6557, Val Loss: 0.6612, Val Acc: 0.5967\n",
      "Best Hyperparameters: {'learning_rate': 0.00483184599739595, 'batch_size': 32, 'hidden_dimension': 512, 'optimizer': 'Adam'}\n",
      "Best model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Trained the model and performing the hyperparameter tuning using validation data\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "# Initializing the  Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "global_best_model_state = None\n",
    "global_best_accuracy = 0  \n",
    "\n",
    "def optimization(trial):\n",
    "    global global_best_model_state, global_best_accuracy \n",
    "    print(f\"Running Trail {trial.number} \\n\")\n",
    "    # defining different hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\",1e-3, 5e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",[32,64])\n",
    "    hidden_dimension= trial.suggest_categorical(\"hidden_dimension\",[128,256,512])\n",
    "    optimizers = trial.suggest_categorical(\"optimizer\",[\"Adam\",\"SGD\",\"AdamW\"])\n",
    "    \n",
    "    print(f\"Selected Hyperparameters: lr={learning_rate}, batch_size={batch_size}, hidden_dim={hidden_dimension}, optimizer={optimizers}\")\n",
    "\n",
    "\n",
    "    # defining the model\n",
    "    model = answerclassifier(1,hidden_dim=hidden_dimension,dropout_prob=0.1,num_heads=4).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # initializing the selected optimizer\n",
    "    if optimizers ==\"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=1e-4)\n",
    "    elif optimizers ==\"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)\n",
    "    elif optimizers ==\"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=1e-4)\n",
    "    \n",
    "    # loading data for different batch sizes\n",
    "    train_loader_ac = DataLoader(train_dataset_ac, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_ac = DataLoader(val_dataset_ac, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_validation_accuracy = 0\n",
    "    best_model_state = None\n",
    "    # Training for five epochs\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        train_loss =0\n",
    "\n",
    "        # training by batch wise\n",
    "        for images,questions,labels in train_loader_ac:\n",
    "            images,questions,labels = images.to(device), questions.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images, questions).squeeze(dim=-1)\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss =0\n",
    "        correct, total = 0,0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images,questions,labels in val_loader_ac:\n",
    "                images,questions,labels = images.to(device), questions.to(device), labels.to(device)\n",
    "                outputs = model(images, questions).squeeze(dim=-1)\n",
    "                loss = criterion(outputs,labels)\n",
    "\n",
    "                val_loss+=loss.item()\n",
    "                # coverting the output logits into 0's and 1's \n",
    "                predicted = torch.sigmoid(outputs).round()\n",
    "           \n",
    "                correct+= (predicted==labels).sum().item()\n",
    "               \n",
    "                total+= labels.size(0)\n",
    "            \n",
    "\n",
    "        # calculating the training and validation loss per each batch \n",
    "        average_train_loss = train_loss / len(train_loader_ac)\n",
    "        average_val_loss = val_loss / len(val_loader_ac)\n",
    "\n",
    "        # calculating the validation accuracy\n",
    "        validation_accuracy = correct/total\n",
    "        print(f\"Epoch [{epoch+1}/{5}] - Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}, Val Acc: {validation_accuracy:.4f}\")\n",
    "\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        # Early Stopping if Validation accuracy is Not Improving\n",
    "        # reporting the loss for pruning\n",
    "        trial.report(validation_accuracy,epoch)\n",
    "\n",
    "        # Stopping the epoch loop early if the validation is not improving much\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    print(f\"Trial {trial.number} Completed!  Val Acc: {best_validation_accuracy:.4f}\\n\")\n",
    "    if best_validation_accuracy > global_best_accuracy:\n",
    "        global_best_accuracy = best_validation_accuracy\n",
    "        global_best_model_state = best_model_state\n",
    "    # returning the validation loss\n",
    "    return best_validation_accuracy\n",
    "\n",
    "# Running the hyperparameter tuning with optuna\n",
    "# aim is to minimize the validation loss\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# Running 10 hyperparameter trials\n",
    "study.optimize(optimization,n_trials=10)\n",
    "\n",
    "print(\"Best Hyperparameters:\",study.best_params)\n",
    "\n",
    "\n",
    "# Saving the Best Model\n",
    "if global_best_model_state is not None:\n",
    "    torch.save(global_best_model_state, \"best_multimodal_classifier.pth\")\n",
    "    print(\"Best model saved successfully!\")\n",
    "else:\n",
    "    print(\"No best model found to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with the best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = answerclassifier(\n",
    "    num_classes=1,\n",
    "    hidden_dim=study.best_params[\"hidden_dimension\"],\n",
    "    dropout_prob=0.1,\n",
    "    num_heads=4\n",
    ").to(device)\n",
    "\n",
    "best_model.load_state_dict(torch.load(\"best_multimodal_classifier.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "test_loader_ac = DataLoader(test_dataset_ac, study.best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, questions in test_loader_ac:\n",
    "        images, questions= images.to(device), questions.to(device)\n",
    "\n",
    "        outputs = best_model(images, questions).squeeze(dim=-1)\n",
    "        preds = torch.sigmoid(outputs).round().int()\n",
    "        predictions.append(preds.cpu()) \n",
    "\n",
    "       \n",
    "final_preds = torch.cat(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_preds, \"Manidatta_Anumandla_challenge1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_choosen_answers,train_category_id2name,train_category_name2id = choosen_answers(train_data)\n",
    "val_choosen_answers,val_category_id2name,val_category_name2id = choosen_answers(val_data)\n",
    "\n",
    "train_target_tensors = Target_tensors(train_choosen_answers,train_category_name2id)\n",
    "val_target_tensors = Target_tensors(val_choosen_answers,val_category_name2id)\n",
    "torch.save(train_target_tensors, \"train_target_tensors.pt\")\n",
    "torch.save(val_target_tensors, \"val_target_tensors.pt\")\n",
    "\n",
    "glove_dict = load_glove_embeddings(\"glove.6B.50d.txt\")\n",
    "embedding_dim = 50\n",
    "\n",
    "train_question_tensors = Question_Tensors(train_data, glove_dict, embedding_dim)\n",
    "val_question_tensors = Question_Tensors(val_data, glove_dict, embedding_dim)\n",
    "test_question_tensors = Question_Tensors(test_data, glove_dict, embedding_dim)\n",
    "\n",
    "torch.save(train_question_tensors, \"train_question_tensors.pt\")\n",
    "torch.save(val_question_tensors, \"val_question_tensors.pt\")#torch.save(test_question_tensors, \"test_question_tensors.pt\")\n",
    "\n",
    "#train_image_tensors = Image_tensors(train_data)\n",
    "#val_image_tensors = Image_tensors(val_data)\n",
    "#test_image_tensors = Image_tensors(test_data)\n",
    "\n",
    "#torch.save(train_image_tensors, \"train_image_tensors.pt\")\n",
    "#torch.save(val_image_tensors, \"val_image_tensors.pt\")\n",
    "#torch.save(test_image_tensors, \"test_image_tensors.pt\")\n",
    "\n",
    "\n",
    "train_image_tensors=torch.load(\"train_image_tensors.pt\")\n",
    "val_image_tensors=torch.load(\"val_image_tensors.pt\")\n",
    "test_image_tensors=torch.load(\"test_image_tensors.pt\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Create a dictionary with all required variables\n",
    "data_to_save = {\n",
    "    \"train_choosen_answers\": train_choosen_answers,\n",
    "    \"train_category_id2name\": train_category_id2name,\n",
    "    \"train_category_name2id\": train_category_name2id,\n",
    "    \"val_choosen_answers\": val_choosen_answers,\n",
    "    \"val_category_id2name\": val_category_id2name,\n",
    "    \"val_category_name2id\": val_category_name2id,\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"category_mappings.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_image_tensors,train_question_tensors,train_target_tensors)\n",
    "val_dataset = TensorDataset(val_image_tensors,val_question_tensors,val_target_tensors)\n",
    "test_dataset = TensorDataset(test_image_tensors,test_question_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        return attn_output\n",
    "\n",
    "class answergeneration(nn.Module):\n",
    "    def __init__(self, num_classes=301, hidden_dim=512, dropout_prob=0.1, num_heads=4):\n",
    "        super(answergeneration, self).__init__()\n",
    "\n",
    "        # Image feature extractor using ResNet18\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        for param in list(self.resnet.parameters())[:-4]:  # freezing the earlier layers\n",
    "            param.requires_grad = False\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hidden_dim)\n",
    "\n",
    "        self.fc_image1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc_image2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Question feature extractor (GloVe embedding dim is 50)\n",
    "        self.fc_question1 = nn.Linear(50, hidden_dim * 2)\n",
    "        self.fc_question2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        # Cross attention\n",
    "        self.cross_attention = CrossAttention(embed_dim=hidden_dim, num_heads=num_heads)\n",
    "\n",
    "        # Fusion + classification\n",
    "        self.fc_fusion1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc_fusion2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, image, question_embedding):\n",
    "        # Image pipeline\n",
    "        x = self.resnet(image)  # output shape: [batch_size, hidden_dim]\n",
    "        image_features = self.dropout(F.relu(self.fc_image1(x)))\n",
    "        image_features = self.dropout(F.relu(self.fc_image2(image_features)))\n",
    "\n",
    "        # Question pipeline (mean of GloVe embeddings per question)\n",
    "        question_features = self.dropout(F.relu(self.fc_question1(question_embedding)))\n",
    "        question_features = self.dropout(F.relu(self.fc_question2(question_features)))\n",
    "\n",
    "        # Add sequence dim for cross-attention\n",
    "        image_seq = image_features.unsqueeze(1)  \n",
    "        question_seq = question_features.unsqueeze(1)  \n",
    "\n",
    "        attended_features = self.cross_attention(query=image_seq, key=question_seq, value=question_seq)\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "\n",
    "        # Fusion and classification\n",
    "        fusion_input = torch.cat((attended_features, image_features), dim=1)\n",
    "        fused = self.dropout(F.relu(self.norm1(self.fc_fusion1(fusion_input))))\n",
    "        fused = self.dropout(F.relu(self.norm2(self.fc_fusion2(fused))))\n",
    "        output = self.fc_output(fused)\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "def evaluate_predictions(predicted_answers,batch_true_answers):\n",
    "    accuracies = []\n",
    "    for predicted_answer, true_answers in zip(predicted_answers, batch_true_answers):\n",
    "        predicted_answer = predicted_answer.lower()\n",
    "        extract_true_answers = [answer['answer'].lower() for answer in true_answers['answers']]\n",
    "        answer_counts = Counter(extract_true_answers)\n",
    "        answer_match= answer_counts.get(predicted_answer, 0)\n",
    "        \n",
    "\n",
    "        accuracy = min(answer_match / 3, 1)\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(accuracies)  if accuracies else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 12:06:56,585] A new study created in memory with name: no-name-4ae57487-cb44-4399-af47-5afee1706e45\n",
      "/var/folders/_h/ngxdgxln57gb923874j8w4rh0000gn/T/ipykernel_1956/3841606497.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-3, 5e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Trial 1 \n",
      "Selected Hyperparameters: lr=0.004113372990308278, batch_size=32, heads=2, hidden_dim=256, optimizer=Adam\n",
      "Epoch [1/5] - Training Loss: 3.5780 - Train accuracy:0.12\n",
      "Epoch [1/5] - validation Loss: 3.5729 - validation accuracy:0.00\n",
      "Epoch [2/5] - Training Loss: 3.1932 - Train accuracy:0.10\n",
      "Epoch [2/5] - validation Loss: 3.5217 - validation accuracy:0.00\n",
      "Epoch [3/5] - Training Loss: 3.0315 - Train accuracy:0.09\n",
      "Epoch [3/5] - validation Loss: 3.4593 - validation accuracy:0.00\n",
      "Epoch [4/5] - Training Loss: 2.9580 - Train accuracy:0.08\n",
      "Epoch [4/5] - validation Loss: 3.3497 - validation accuracy:0.40\n",
      "Epoch [5/5] - Training Loss: 2.9272 - Train accuracy:0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 12:08:30,014] Trial 0 finished with value: 0.4034722222222221 and parameters: {'learning_rate': 0.004113372990308278, 'batch_size': 32, 'num_heads': 2, 'hidden_dimension': 256, 'optimizers': 'Adam'}. Best is trial 0 with value: 0.4034722222222221.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - validation Loss: 3.5391 - validation accuracy:0.20\n",
      "Trial 1 Completed! Final Validation Accuracy: 0.1983\n",
      "\n",
      "\n",
      " Running Trial 2 \n",
      "Selected Hyperparameters: lr=0.0024903564969056846, batch_size=16, heads=4, hidden_dim=512, optimizer=Adam\n",
      "Epoch [1/5] - Training Loss: 3.7087 - Train accuracy:0.13\n",
      "Epoch [1/5] - validation Loss: 3.3204 - validation accuracy:0.55\n",
      "Epoch [2/5] - Training Loss: 3.2267 - Train accuracy:0.09\n",
      "Epoch [2/5] - validation Loss: 3.5874 - validation accuracy:0.00\n",
      "Epoch [3/5] - Training Loss: 3.0710 - Train accuracy:0.09\n",
      "Epoch [3/5] - validation Loss: 3.4517 - validation accuracy:0.00\n",
      "Epoch [4/5] - Training Loss: 2.9890 - Train accuracy:0.08\n",
      "Epoch [4/5] - validation Loss: 3.4728 - validation accuracy:0.01\n",
      "Epoch [5/5] - Training Loss: 2.9412 - Train accuracy:0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 12:10:21,003] Trial 1 finished with value: 0.5533625730994153 and parameters: {'learning_rate': 0.0024903564969056846, 'batch_size': 16, 'num_heads': 4, 'hidden_dimension': 512, 'optimizers': 'Adam'}. Best is trial 1 with value: 0.5533625730994153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - validation Loss: 3.6216 - validation accuracy:0.32\n",
      "Trial 2 Completed! Final Validation Accuracy: 0.3194\n",
      "\n",
      "\n",
      " Running Trial 3 \n",
      "Selected Hyperparameters: lr=0.0010586066394518549, batch_size=32, heads=2, hidden_dim=512, optimizer=Adam\n",
      "Epoch [1/5] - Training Loss: 3.5113 - Train accuracy:0.14\n",
      "Epoch [1/5] - validation Loss: 3.3674 - validation accuracy:0.39\n",
      "Epoch [2/5] - Training Loss: 3.1281 - Train accuracy:0.10\n",
      "Epoch [2/5] - validation Loss: 3.5858 - validation accuracy:0.00\n",
      "Epoch [3/5] - Training Loss: 3.0250 - Train accuracy:0.09\n",
      "Epoch [3/5] - validation Loss: 3.3818 - validation accuracy:0.01\n",
      "Epoch [4/5] - Training Loss: 2.9491 - Train accuracy:0.11\n",
      "Epoch [4/5] - validation Loss: 3.4187 - validation accuracy:0.29\n",
      "Epoch [5/5] - Training Loss: 2.8881 - Train accuracy:0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 12:11:53,822] Trial 2 finished with value: 0.3868055555555555 and parameters: {'learning_rate': 0.0010586066394518549, 'batch_size': 32, 'num_heads': 2, 'hidden_dimension': 512, 'optimizers': 'Adam'}. Best is trial 1 with value: 0.5533625730994153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - validation Loss: 3.4606 - validation accuracy:0.28\n",
      "Trial 3 Completed! Final Validation Accuracy: 0.2774\n",
      "\n",
      "\n",
      " Running Trial 4 \n",
      "Selected Hyperparameters: lr=0.0011997741872213214, batch_size=16, heads=4, hidden_dim=256, optimizer=SGD\n",
      "Epoch [1/5] - Training Loss: 3.7736 - Train accuracy:0.12\n",
      "Epoch [1/5] - validation Loss: 3.5471 - validation accuracy:0.41\n",
      "Epoch [2/5] - Training Loss: 3.2711 - Train accuracy:0.11\n",
      "Epoch [2/5] - validation Loss: 3.5703 - validation accuracy:0.14\n",
      "Epoch [3/5] - Training Loss: 3.1864 - Train accuracy:0.10\n",
      "Epoch [3/5] - validation Loss: 3.4069 - validation accuracy:0.23\n",
      "Epoch [4/5] - Training Loss: 3.1367 - Train accuracy:0.10\n",
      "Epoch [4/5] - validation Loss: 3.5588 - validation accuracy:0.01\n",
      "Epoch [5/5] - Training Loss: 3.0922 - Train accuracy:0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 12:13:40,923] Trial 3 finished with value: 0.4097222222222222 and parameters: {'learning_rate': 0.0011997741872213214, 'batch_size': 16, 'num_heads': 4, 'hidden_dimension': 256, 'optimizers': 'SGD'}. Best is trial 1 with value: 0.5533625730994153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - validation Loss: 3.5043 - validation accuracy:0.09\n",
      "Trial 4 Completed! Final Validation Accuracy: 0.0881\n",
      "\n",
      "\n",
      " Running Trial 5 \n",
      "Selected Hyperparameters: lr=0.0012262424348634532, batch_size=32, heads=2, hidden_dim=256, optimizer=AdamW\n",
      "Epoch [1/5] - Training Loss: 3.4675 - Train accuracy:0.12\n",
      "Epoch [1/5] - validation Loss: 3.4534 - validation accuracy:0.11\n",
      "Epoch [2/5] - Training Loss: 3.1218 - Train accuracy:0.09\n",
      "Epoch [2/5] - validation Loss: 3.3755 - validation accuracy:0.30\n",
      "Epoch [3/5] - Training Loss: 2.9927 - Train accuracy:0.12\n",
      "Epoch [3/5] - validation Loss: 3.4871 - validation accuracy:0.18\n",
      "Epoch [4/5] - Training Loss: 2.9112 - Train accuracy:0.10\n",
      "Epoch [4/5] - validation Loss: 3.5424 - validation accuracy:0.24\n",
      "Epoch [5/5] - Training Loss: 2.8397 - Train accuracy:0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 12:15:13,917] Trial 4 finished with value: 0.3024305555555556 and parameters: {'learning_rate': 0.0012262424348634532, 'batch_size': 32, 'num_heads': 2, 'hidden_dimension': 256, 'optimizers': 'AdamW'}. Best is trial 1 with value: 0.5533625730994153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] - validation Loss: 3.6709 - validation accuracy:0.23\n",
      "Trial 5 Completed! Final Validation Accuracy: 0.2337\n",
      "\n",
      "\n",
      " Best Hyperparameters Found:\n",
      "{'learning_rate': 0.0024903564969056846, 'batch_size': 16, 'num_heads': 4, 'hidden_dimension': 512, 'optimizers': 'Adam'}\n",
      "Best model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "global_best_model_state = None\n",
    "global_best_accuracy = 0 \n",
    "\n",
    "def optimizer(trial):\n",
    "    global global_best_model_state, global_best_accuracy\n",
    "    print(f\"\\n Running Trial {trial.number + 1} \")\n",
    "\n",
    "    # Suggesting the optimizer\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-3, 5e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16,32])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2,4])\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_dimension\", [256, 512])\n",
    "    optimizers = trial.suggest_categorical(\"optimizers\", [\"Adam\",\"SGD\",\"AdamW\"])\n",
    "\n",
    "    print(f\"Selected Hyperparameters: lr={learning_rate}, batch_size={batch_size}, heads={num_heads}, hidden_dim={hidden_size}, optimizer={optimizers}\")\n",
    "\n",
    "    # Initializing the Model\n",
    "    model = answergeneration(301,hidden_size,dropout_prob=0.1,num_heads=num_heads).to(device)\n",
    "\n",
    "    model.apply(initialize_weights)\n",
    "    #class_weights = get_class_weights(train_target_tensors).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # Choosing the Optimizer\n",
    "    if optimizers == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-4)\n",
    "    elif optimizers == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    best_validation_accuracy = 0\n",
    "    best_model_state = None \n",
    "    # Training loop for 5 epochs in each hyperparameter tuning trial\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        # passing the model into training mode\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_accuracy =0\n",
    "        for batch_idx, (images, questions, answers) in enumerate(train_loader):\n",
    "            images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images, questions)\n",
    "           \n",
    "            loss = criterion(output, answers.long())\n",
    "            train_loss+= loss.item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(output, dim=1)  \n",
    "            predicted = predicted.cpu().numpy()\n",
    "            predicted_answers = [train_category_id2name[pred] for pred in predicted if pred in train_category_id2name]\n",
    "            train_batch_accuracy = evaluate_predictions(predicted_answers,train_data[batch_idx * answers.size(0) : (batch_idx + 1) * answers.size(0)])\n",
    "            train_accuracy += train_batch_accuracy \n",
    "            \n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_accuracy / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {train_loss:.4f} - Train accuracy:{train_accuracy:.2f}\")\n",
    "\n",
    "        # Validating the model using validation data\n",
    "        # passing the model into evaluation mode\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy =0\n",
    "        # running the loop with any gradients calcualtion\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, questions, answers) in enumerate(val_loader):\n",
    "                images, questions, answers = images.to(device),questions.to(device),answers.to(device)\n",
    "                output = model(images, questions)\n",
    "                loss = criterion(output, answers.long())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _,predicted_labels = torch.max(output, dim=1)\n",
    "                predicted_labels = predicted_labels.cpu().numpy()\n",
    "                predicted_answers = [val_category_id2name[pred] for pred in predicted_labels if pred in val_category_id2name]\n",
    "                val_batch_accuracy = evaluate_predictions(predicted_answers,val_data[batch_idx * answers.size(0) : (batch_idx + 1) * answers.size(0)])\n",
    "                val_accuracy += val_batch_accuracy \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_accuracy /len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - validation Loss: {val_loss:.4f} - validation accuracy:{val_accuracy:.2f}\")\n",
    "        if val_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "        # Early Stopping & Pruning\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            print(\" Trial Pruned Due to No Improvement!\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    print(f\"Trial {trial.number + 1} Completed! Final Validation Accuracy: {val_accuracy :.4f}\\n\")\n",
    "    if best_validation_accuracy > global_best_accuracy:\n",
    "        global_best_accuracy = best_validation_accuracy\n",
    "        global_best_model_state = best_model_state\n",
    "    \n",
    "    return best_validation_accuracy\n",
    "\n",
    "# Running the Optuna Hyperparameter Tuning aiming to minimize the validation loss\n",
    "study = optuna.create_study(direction=\"maximize\") \n",
    "# running the tuning for ten 10 which takes different combination of hyperparameters each time\n",
    "study.optimize(optimizer, n_trials=5) \n",
    "\n",
    "# Best Hyperparameters\n",
    "print(\"\\n Best Hyperparameters Found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Saving the Best Model\n",
    "if global_best_model_state is not None:\n",
    "    torch.save(global_best_model_state, \"best_multimodal_answerable.pth\")\n",
    "    print(\"Best model saved successfully!\")\n",
    "else:\n",
    "    print(\"No best model found to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['grey', 'other_categories', 'grey', 'other_categories', 'other_categories', 'unanswerable', 'other_categories', 'other_categories', 'other_categories', 'grey', 'unanswerable', 'other_categories', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable'], ['other_categories', 'unanswerable', 'unanswerable', 'grey', 'unanswerable', 'other_categories', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'other_categories', 'unanswerable', 'unanswerable', 'unanswerable', 'grey'], ['unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'other_categories', 'other_categories', 'other_categories', 'unanswerable', 'unanswerable', 'unanswerable', 'other_categories', 'other_categories', 'unanswerable', 'grey', 'unanswerable'], ['other_categories', 'other_categories', 'unanswerable', 'other_categories', 'unanswerable', 'grey', 'unanswerable', 'other_categories', 'grey', 'unanswerable', 'other_categories', 'unanswerable', 'unanswerable', 'other_categories', 'other_categories', 'other_categories'], ['grey', 'grey', 'other_categories', 'unanswerable', 'grey', 'other_categories', 'other_categories', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'other_categories', 'other_categories', 'unanswerable', 'other_categories'], ['unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'unanswerable', 'other_categories', 'unanswerable', 'unanswerable', 'unanswerable', 'grey', 'unanswerable', 'other_categories', 'other_categories', 'other_categories', 'unanswerable', 'grey'], ['other_categories', 'grey', 'other_categories', 'other_categories']]\n",
      "Submission file saved as: Manidatta_Anumandla_challenge2.json\n"
     ]
    }
   ],
   "source": [
    "best_model = answergeneration(\n",
    "   num_classes=301,\n",
    "   hidden_dim=study.best_params[\"hidden_dimension\"],\n",
    "   dropout_prob=0.1,\n",
    "   num_heads=study.best_params[\"num_heads\"]\n",
    ").to(device)\n",
    "\n",
    "best_model.load_state_dict(torch.load(\"best_multimodal_answerable.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, study.best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, questions in test_loader:\n",
    "        images, questions= images.to(device), questions.to(device)\n",
    "\n",
    "        outputs = best_model(images, questions).squeeze(dim=-1)\n",
    "        _,predicted_labels = torch.max(outputs, dim=1)\n",
    "        predicted_labels = predicted_labels.cpu().numpy()\n",
    "        predicted_answers = [train_category_id2name[pred] for pred in predicted_labels if pred in train_category_id2name]\n",
    "        predictions.append(predicted_answers)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "flat_predictions = [answer for batch in predictions for answer in batch]\n",
    "\n",
    "# Ensuring the lengths match\n",
    "assert len(top_100_image_urls) == len(flat_predictions), \"Mismatch in image URLs and predictions count!\"\n",
    "\n",
    "# Creating the  JSON structure\n",
    "submission_data = [{\"image\": img_url, \"answer\": pred} for img_url, pred in zip(top_100_image_urls, flat_predictions)]\n",
    "\n",
    "# Saving to JSON file\n",
    "submission_filename = \"Manidatta_Anumandla_challenge2.json\"\n",
    "with open(submission_filename, \"w\") as json_file:\n",
    "    json.dump(submission_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Submission file saved as: {submission_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predictions from the JSON file:\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000000.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000001.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000002.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000003.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000004.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000005.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000006.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000007.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000008.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000009.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000010.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000011.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000012.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000013.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000014.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000015.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000016.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000017.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000018.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000019.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000020.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000021.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000022.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000023.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000024.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000025.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000026.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000027.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000028.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000029.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000030.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000031.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000032.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000033.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000034.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000035.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000036.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000037.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000038.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000039.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000040.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000041.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000042.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000043.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000044.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000045.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000046.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000047.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000048.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000049.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000050.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000051.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000052.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000053.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000054.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000055.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000056.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000057.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000058.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000059.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000060.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000061.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000062.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000063.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000064.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000065.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000066.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000067.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000068.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000069.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000070.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000071.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000072.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000073.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000074.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000075.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000076.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000077.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000078.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000079.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000080.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000081.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000082.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000083.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000084.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000085.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000086.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000087.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000088.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000089.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000090.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000091.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000092.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000093.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000094.jpg', 'answer': 'unanswerable'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000095.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000096.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000097.jpg', 'answer': 'grey'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000098.jpg', 'answer': 'other_categories'}\n",
      "{'image': 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/VizWiz_test_00000099.jpg', 'answer': 'other_categories'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON file\n",
    "submission_filename = \"Manidatta_Anumandla_challenge2.json\"\n",
    "\n",
    "with open(submission_filename, \"r\") as json_file:\n",
    "    submission_data = json.load(json_file)\n",
    "\n",
    "# Display first few entries\n",
    "print(\"First 5 predictions from the JSON file:\")\n",
    "for entry in submission_data[:]:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"Manidatta_Anumandla_challenge1.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
